"""
Implementa√ß√£o COMPLETA de m√©tricas customizadas para avalia√ß√£o de prompts.
RESOLU√á√ÉO DO DESAFIO

Este m√≥dulo implementa m√©tricas gerais e espec√≠ficas para Bug to User Story:

M√âTRICAS GERAIS (3):
1. F1-Score: Balanceamento entre Precision e Recall
2. Clarity: Clareza e estrutura da resposta
3. Precision: Informa√ß√µes corretas e relevantes

M√âTRICAS ESPEC√çFICAS PARA BUG TO USER STORY (4):
4. Tone Score: Tom profissional e emp√°tico
5. Acceptance Criteria Score: Qualidade dos crit√©rios de aceita√ß√£o
6. User Story Format Score: Formato correto (Como... Eu quero... Para que...)
7. Completeness Score: Completude e contexto t√©cnico

Suporta m√∫ltiplos providers de LLM:
- OpenAI (gpt-4o, gpt-4o-mini)
- Google Gemini (gemini-1.5-flash, gemini-1.5-pro)

Configure o provider no arquivo .env atrav√©s da vari√°vel LLM_PROVIDER.
"""

import os
import json
import re
from typing import Dict, Any
from dotenv import load_dotenv
from langchain_core.messages import SystemMessage, HumanMessage

try:
    from .utils import get_eval_llm, invoke_with_throttle_retry
except ImportError:
    from utils import get_eval_llm, invoke_with_throttle_retry

load_dotenv()


def get_evaluator_llm():
    """
    Retorna o LLM configurado para avalia√ß√£o.
    Suporta OpenAI e Google Gemini baseado no .env
    """
    return get_eval_llm(temperature=0)


def extract_json_from_response(response_text: str) -> Dict[str, Any]:
    """
    Extrai JSON de uma resposta de LLM que pode conter texto adicional.
    """
    try:
        # Tentar parsear diretamente
        return json.loads(response_text)
    except json.JSONDecodeError:
        # Tentar encontrar JSON no meio do texto
        start = response_text.find('{')
        end = response_text.rfind('}') + 1

        if start != -1 and end > start:
            try:
                json_str = response_text[start:end]
                return json.loads(json_str)
            except json.JSONDecodeError:
                pass

        # Se n√£o conseguir extrair, retornar valores default
        print(f"‚ö†Ô∏è  N√£o foi poss√≠vel extrair JSON da resposta: {response_text[:200]}...")
        return {"score": 0.0, "reasoning": "Erro ao processar resposta"}


def evaluate_f1_score(question: str, answer: str, reference: str) -> Dict[str, Any]:
    """
    Calcula F1-Score usando LLM-as-Judge.

    F1-Score = 2 * (Precision * Recall) / (Precision + Recall)

    Args:
        question: Pergunta feita pelo usu√°rio
        answer: Resposta gerada pelo prompt
        reference: Resposta esperada (ground truth)

    Returns:
        Dict com score e reasoning:
        {
            "score": 0.95,
            "precision": 0.9,
            "recall": 0.99,
            "reasoning": "Explica√ß√£o do LLM..."
        }
    """
    evaluator_prompt = f"""
Voc√™ √© um avaliador especializado em medir a qualidade de respostas geradas por IA.

Sua tarefa √© calcular PRECISION e RECALL para determinar o F1-Score.

PERGUNTA DO USU√ÅRIO:
{question}

RESPOSTA ESPERADA (Ground Truth):
{reference}

RESPOSTA GERADA PELO MODELO:
{answer}

INSTRU√á√ïES:

1. PRECISION (0.0 a 1.0):
   - Quantas informa√ß√µes na resposta gerada s√£o CORRETAS e RELEVANTES?
   - Penalizar informa√ß√µes incorretas, inventadas ou desnecess√°rias
   - 1.0 = todas informa√ß√µes s√£o corretas e relevantes
   - 0.0 = nenhuma informa√ß√£o √© correta ou relevante

2. RECALL (0.0 a 1.0):
   - Quantas informa√ß√µes da resposta esperada est√£o PRESENTES na resposta gerada?
   - Penalizar informa√ß√µes importantes que foram omitidas
   - 1.0 = todas informa√ß√µes importantes est√£o presentes
   - 0.0 = nenhuma informa√ß√£o importante est√° presente

3. RACIOC√çNIO:
   - Explique brevemente sua avalia√ß√£o
   - Cite exemplos espec√≠ficos do que estava correto/incorreto

IMPORTANTE: Retorne APENAS um objeto JSON v√°lido no formato:
{{
  "precision": <valor entre 0.0 e 1.0>,
  "recall": <valor entre 0.0 e 1.0>,
  "reasoning": "<sua explica√ß√£o em at√© 100 palavras>"
}}

N√ÉO adicione nenhum texto antes ou depois do JSON.
"""

    try:
        llm = get_evaluator_llm()
        response = invoke_with_throttle_retry(
            lambda: llm.invoke([HumanMessage(content=evaluator_prompt)]),
            context="evaluate_f1_score"
        )
        result = extract_json_from_response(response.content)

        precision = float(result.get("precision", 0.0))
        recall = float(result.get("recall", 0.0))

        # Calcular F1-Score
        if (precision + recall) > 0:
            f1_score = 2 * (precision * recall) / (precision + recall)
        else:
            f1_score = 0.0

        return {
            "score": round(f1_score, 4),
            "precision": round(precision, 4),
            "recall": round(recall, 4),
            "reasoning": result.get("reasoning", "")
        }

    except Exception as e:
        print(f"‚ùå Erro ao avaliar F1-Score: {e}")
        return {
            "score": 0.0,
            "precision": 0.0,
            "recall": 0.0,
            "reasoning": f"Erro na avalia√ß√£o: {str(e)}"
        }


def evaluate_clarity(question: str, answer: str, reference: str) -> Dict[str, Any]:
    """
    Avalia a clareza e estrutura da resposta usando LLM-as-Judge.

    Crit√©rios:
    - Organiza√ß√£o e estrutura clara
    - Linguagem simples e direta
    - Aus√™ncia de ambiguidade
    - F√°cil de entender

    Args:
        question: Pergunta feita pelo usu√°rio
        answer: Resposta gerada pelo prompt
        reference: Resposta esperada (ground truth)

    Returns:
        Dict com score e reasoning:
        {
            "score": 0.92,
            "reasoning": "Explica√ß√£o do LLM..."
        }
    """
    evaluator_prompt = f"""
Voc√™ √© um avaliador especializado em medir a CLAREZA de respostas geradas por IA.

PERGUNTA DO USU√ÅRIO:
{question}

RESPOSTA GERADA PELO MODELO:
{answer}

RESPOSTA ESPERADA (Refer√™ncia):
{reference}

INSTRU√á√ïES:

Avalie a CLAREZA da resposta gerada com base nos crit√©rios:

1. ORGANIZA√á√ÉO (0.0 a 1.0):
   - A resposta tem estrutura l√≥gica e bem organizada?
   - Informa√ß√µes est√£o em ordem sensata?

2. LINGUAGEM (0.0 a 1.0):
   - Usa linguagem simples e direta?
   - Evita jarg√µes desnecess√°rios?
   - F√°cil de entender?

3. AUS√äNCIA DE AMBIGUIDADE (0.0 a 1.0):
   - A resposta √© clara e sem ambiguidades?
   - N√£o deixa d√∫vidas sobre o que est√° sendo comunicado?

4. CONCIS√ÉO (0.0 a 1.0):
   - √â concisa sem ser curta demais?
   - N√£o tem informa√ß√µes redundantes?

Calcule a M√âDIA dos 4 crit√©rios para obter o score final.

IMPORTANTE: Retorne APENAS um objeto JSON v√°lido no formato:
{{
  "score": <valor entre 0.0 e 1.0>,
  "reasoning": "<explica√ß√£o detalhada da avalia√ß√£o em at√© 100 palavras>"
}}

N√ÉO adicione nenhum texto antes ou depois do JSON.
"""

    try:
        llm = get_evaluator_llm()
        response = invoke_with_throttle_retry(
            lambda: llm.invoke([HumanMessage(content=evaluator_prompt)]),
            context="evaluate_clarity"
        )
        result = extract_json_from_response(response.content)

        score = float(result.get("score", 0.0))

        return {
            "score": round(score, 4),
            "reasoning": result.get("reasoning", "")
        }

    except Exception as e:
        print(f"‚ùå Erro ao avaliar Clarity: {e}")
        return {
            "score": 0.0,
            "reasoning": f"Erro na avalia√ß√£o: {str(e)}"
        }


def evaluate_precision(question: str, answer: str, reference: str) -> Dict[str, Any]:
    """
    Avalia a precis√£o da resposta usando LLM-as-Judge.

    Crit√©rios:
    - Aus√™ncia de informa√ß√µes inventadas (alucina√ß√µes)
    - Resposta focada na pergunta
    - Informa√ß√µes corretas e verific√°veis

    Args:
        question: Pergunta feita pelo usu√°rio
        answer: Resposta gerada pelo prompt
        reference: Resposta esperada (ground truth)

    Returns:
        Dict com score e reasoning:
        {
            "score": 0.98,
            "reasoning": "Explica√ß√£o do LLM..."
        }
    """
    
    evaluator_prompt = f"""
Voc√™ √© um avaliador especializado em detectar PRECIS√ÉO e ALUCINA√á√ïES em respostas de IA.

PERGUNTA DO USU√ÅRIO:
{question}

RESPOSTA GERADA PELO MODELO:
{answer}

RESPOSTA ESPERADA (Ground Truth):
{reference}

INSTRU√á√ïES:

Avalie a PRECIS√ÉO da resposta gerada:

1. AUS√äNCIA DE ALUCINA√á√ïES (0.0 a 1.0):
   - A resposta cont√©m informa√ß√µes INVENTADAS ou n√£o verific√°veis?
   - Todas as afirma√ß√µes s√£o baseadas em fatos?
   - 1.0 = nenhuma alucina√ß√£o detectada
   - 0.0 = resposta cheia de informa√ß√µes inventadas

2. FOCO NA PERGUNTA (0.0 a 1.0):
   - A resposta responde EXATAMENTE o que foi perguntado?
   - N√£o divaga ou adiciona informa√ß√µes n√£o solicitadas?
   - 1.0 = totalmente focada
   - 0.0 = completamente fora do t√≥pico

3. CORRE√á√ÉO FACTUAL (0.0 a 1.0):
   - As informa√ß√µes est√£o CORRETAS quando comparadas com a refer√™ncia?
   - N√£o h√° erros ou imprecis√µes?
   - 1.0 = todas informa√ß√µes corretas
   - 0.0 = informa√ß√µes incorretas

Calcule a M√âDIA dos 3 crit√©rios para obter o score final.

IMPORTANTE: Retorne APENAS um objeto JSON v√°lido no formato:
{{
  "score": <valor entre 0.0 e 1.0>,
  "reasoning": "<explica√ß√£o detalhada em at√© 100 palavras, cite exemplos>"
}}

N√ÉO adicione nenhum texto antes ou depois do JSON.
"""

    try:
        llm = get_evaluator_llm()
        response = invoke_with_throttle_retry(
            lambda: llm.invoke([HumanMessage(content=evaluator_prompt)]),
            context="evaluate_precision"
        )
        result = extract_json_from_response(response.content)

        score = float(result.get("score", 0.0))

        return {
            "score": round(score, 4),
            "reasoning": result.get("reasoning", "")
        }

    except Exception as e:
        print(f"‚ùå Erro ao avaliar Precision: {e}")
        return {
            "score": 0.0,
            "reasoning": f"Erro na avalia√ß√£o: {str(e)}"
        }


def evaluate_tone_score(bug_report: str, user_story: str, reference: str) -> Dict[str, Any]:
    """
    Avalia o tom da user story (profissional e emp√°tico).

    Crit√©rios espec√≠ficos para Bug to User Story:
    - Tom profissional mas n√£o excessivamente t√©cnico
    - Empatia com o usu√°rio afetado pelo bug
    - Foco em valor de neg√≥cio, n√£o apenas corre√ß√£o t√©cnica
    - Linguagem positiva (o que o usu√°rio QUER fazer, n√£o s√≥ o que n√£o funciona)

    Args:
        bug_report: Descri√ß√£o do bug original
        user_story: User story gerada pelo prompt
        reference: User story esperada (ground truth)

    Returns:
        Dict com score e reasoning
    """
    evaluator_prompt = f"""
Voc√™ √© um avaliador especializado em User Stories √°geis.

BUG REPORT ORIGINAL:
{bug_report}

USER STORY GERADA:
{user_story}

USER STORY ESPERADA (Refer√™ncia):
{reference}

INSTRU√á√ïES:

Avalie o TOM da user story gerada com base nos crit√©rios:

1. PROFISSIONALISMO (0.0 a 1.0):
   - Usa linguagem profissional e apropriada para documenta√ß√£o?
   - Evita jarg√µes excessivos ou linguagem muito informal?
   - Mant√©m padr√£o de qualidade de documenta√ß√£o √°gil?

2. EMPATIA COM USU√ÅRIO (0.0 a 1.0):
   - Demonstra compreens√£o do impacto do bug no usu√°rio?
   - Foca na necessidade/frustra√ß√£o do usu√°rio?
   - Usa linguagem centrada no usu√°rio ("Como um... eu quero...")?

3. FOCO EM VALOR (0.0 a 1.0):
   - Articula claramente o valor de neg√≥cio da solu√ß√£o?
   - Vai al√©m de "consertar o bug" e explica o benef√≠cio?
   - Usa a estrutura "para que eu possa..." com valor real?

4. LINGUAGEM POSITIVA (0.0 a 1.0):
   - Foca no que o usu√°rio QUER fazer (n√£o s√≥ no que est√° quebrado)?
   - Tom construtivo e orientado a solu√ß√£o?
   - Evita linguagem negativa ou culpabilizante?

Calcule a M√âDIA dos 4 crit√©rios para obter o score final.

IMPORTANTE: Retorne APENAS um objeto JSON v√°lido no formato:
{{
  "score": <valor entre 0.0 e 1.0>,
  "reasoning": "<explica√ß√£o detalhada em at√© 150 palavras>"
}}

N√ÉO adicione nenhum texto antes ou depois do JSON.
"""

    try:
        llm = get_evaluator_llm()
        response = invoke_with_throttle_retry(
            lambda: llm.invoke([HumanMessage(content=evaluator_prompt)]),
            context="evaluate_tone_score"
        )
        result = extract_json_from_response(response.content)

        score = float(result.get("score", 0.0))

        return {
            "score": round(score, 4),
            "reasoning": result.get("reasoning", "")
        }

    except Exception as e:
        print(f"‚ùå Erro ao avaliar Tone Score: {e}")
        return {
            "score": 0.0,
            "reasoning": f"Erro na avalia√ß√£o: {str(e)}"
        }


def evaluate_acceptance_criteria_score(bug_report: str, user_story: str, reference: str) -> Dict[str, Any]:
    """
    Avalia a qualidade dos crit√©rios de aceita√ß√£o.

    Crit√©rios espec√≠ficos:
    - Usa formato Given-When-Then ou similar estruturado
    - Crit√©rios s√£o espec√≠ficos e test√°veis
    - Quantidade adequada (3-7 crit√©rios idealmente)
    - Cobertura completa do bug e solu√ß√£o
    - Incluem cen√°rios de edge case quando relevante

    Args:
        bug_report: Descri√ß√£o do bug original
        user_story: User story gerada pelo prompt
        reference: User story esperada (ground truth)

    Returns:
        Dict com score e reasoning
    """
    evaluator_prompt = f"""
Voc√™ √© um avaliador especializado em Crit√©rios de Aceita√ß√£o de User Stories.

BUG REPORT ORIGINAL:
{bug_report}

USER STORY GERADA:
{user_story}

USER STORY ESPERADA (Refer√™ncia):
{reference}

INSTRU√á√ïES:

Avalie os CRIT√âRIOS DE ACEITA√á√ÉO da user story gerada:

1. FORMATO ESTRUTURADO (0.0 a 1.0):
   - Usa formato Given-When-Then ou estrutura similar?
   - Cada crit√©rio √© claramente separado e identific√°vel?
   - Formata√ß√£o facilita leitura e entendimento?

2. ESPECIFICIDADE E TESTABILIDADE (0.0 a 1.0):
   - Crit√©rios s√£o espec√≠ficos e n√£o vagos?
   - √â poss√≠vel criar testes automatizados a partir deles?
   - Evita termos amb√≠guos como "deve funcionar bem"?
   - Crit√©rios mensur√°veis e verific√°veis?

3. QUANTIDADE ADEQUADA (0.0 a 1.0):
   - Tem quantidade apropriada de crit√©rios (nem muito, nem pouco)?
   - Ideal: 3-7 crit√©rios para bugs simples/m√©dios
   - Bugs complexos podem ter mais crit√©rios organizados

4. COBERTURA COMPLETA (0.0 a 1.0):
   - Cobre todos os aspectos do bug?
   - Inclui cen√°rios de sucesso e erro?
   - Considera edge cases quando relevante?
   - Aborda valida√ß√µes e requisitos t√©cnicos do bug?

Calcule a M√âDIA dos 4 crit√©rios para obter o score final.

IMPORTANTE: Retorne APENAS um objeto JSON v√°lido no formato:
{{
  "score": <valor entre 0.0 e 1.0>,
  "reasoning": "<explica√ß√£o detalhada com exemplos espec√≠ficos, at√© 150 palavras>"
}}

N√ÉO adicione nenhum texto antes ou depois do JSON.
"""

    try:
        llm = get_evaluator_llm()
        response = invoke_with_throttle_retry(
            lambda: llm.invoke([HumanMessage(content=evaluator_prompt)]),
            context="evaluate_acceptance_criteria_score"
        )
        result = extract_json_from_response(response.content)

        score = float(result.get("score", 0.0))

        return {
            "score": round(score, 4),
            "reasoning": result.get("reasoning", "")
        }

    except Exception as e:
        print(f"‚ùå Erro ao avaliar Acceptance Criteria Score: {e}")
        return {
            "score": 0.0,
            "reasoning": f"Erro na avalia√ß√£o: {str(e)}"
        }


def evaluate_user_story_format_score(bug_report: str, user_story: str, reference: str) -> Dict[str, Any]:
    """
    Avalia se a user story segue o formato padr√£o correto.

    Formato esperado:
    - "Como um [tipo de usu√°rio]"
    - "Eu quero [a√ß√£o/funcionalidade]"
    - "Para que [benef√≠cio/valor]"
    - Crit√©rios de Aceita√ß√£o claramente separados

    Args:
        bug_report: Descri√ß√£o do bug original
        user_story: User story gerada pelo prompt
        reference: User story esperada (ground truth)

    Returns:
        Dict com score e reasoning
    """
    evaluator_prompt = f"""
Voc√™ √© um avaliador especializado em formato de User Stories √°geis.

BUG REPORT ORIGINAL:
{bug_report}

USER STORY GERADA:
{user_story}

USER STORY ESPERADA (Refer√™ncia):
{reference}

INSTRU√á√ïES:

Avalie o FORMATO da user story gerada:

1. TEMPLATE PADR√ÉO (0.0 a 1.0):
   - Segue o formato "Como um [usu√°rio], eu quero [a√ß√£o], para que [benef√≠cio]"?
   - Todas as tr√™s partes est√£o presentes e corretas?
   - Ordem e estrutura seguem as melhores pr√°ticas?

2. IDENTIFICA√á√ÉO DE PERSONA (0.0 a 1.0):
   - "Como um..." identifica claramente o tipo de usu√°rio?
   - Persona √© espec√≠fica e relevante para o bug?
   - Evita gen√©ricos como "Como um usu√°rio" sem contexto?

3. A√á√ÉO CLARA (0.0 a 1.0):
   - "Eu quero..." descreve claramente a a√ß√£o/funcionalidade desejada?
   - A√ß√£o √© espec√≠fica e relacionada ao bug?
   - Evita descri√ß√µes vagas ou muito t√©cnicas?

4. BENEF√çCIO ARTICULADO (0.0 a 1.0):
   - "Para que..." explica claramente o valor/benef√≠cio?
   - Benef√≠cio √© real e significativo (n√£o trivial)?
   - Conecta a a√ß√£o ao valor de neg√≥cio?

5. SEPARA√á√ÉO DE SE√á√ïES (0.0 a 1.0):
   - User story principal est√° claramente separada dos crit√©rios?
   - Crit√©rios de aceita√ß√£o t√™m se√ß√£o pr√≥pria?
   - Estrutura facilita leitura e navega√ß√£o?

Calcule a M√âDIA dos 5 crit√©rios para obter o score final.

IMPORTANTE: Retorne APENAS um objeto JSON v√°lido no formato:
{{
  "score": <valor entre 0.0 e 1.0>,
  "reasoning": "<explica√ß√£o detalhada com exemplos, at√© 150 palavras>"
}}

N√ÉO adicione nenhum texto antes ou depois do JSON.
"""

    try:
        llm = get_evaluator_llm()
        response = invoke_with_throttle_retry(
            lambda: llm.invoke([HumanMessage(content=evaluator_prompt)]),
            context="evaluate_user_story_format_score"
        )
        result = extract_json_from_response(response.content)

        score = float(result.get("score", 0.0))

        return {
            "score": round(score, 4),
            "reasoning": result.get("reasoning", "")
        }

    except Exception as e:
        print(f"‚ùå Erro ao avaliar User Story Format Score: {e}")
        return {
            "score": 0.0,
            "reasoning": f"Erro na avalia√ß√£o: {str(e)}"
        }


def evaluate_completeness_score(bug_report: str, user_story: str, reference: str) -> Dict[str, Any]:
    """
    Avalia a completude da user story em rela√ß√£o ao bug.

    Crit√©rios espec√≠ficos baseados na complexidade do bug:
    - Bugs simples: cobre o problema b√°sico
    - Bugs m√©dios: inclui contexto t√©cnico relevante
    - Bugs complexos: aborda m√∫ltiplos aspectos, impacto, tasks t√©cnicas

    Args:
        bug_report: Descri√ß√£o do bug original
        user_story: User story gerada pelo prompt
        reference: User story esperada (ground truth)

    Returns:
        Dict com score e reasoning
    """
    evaluator_prompt = f"""
Voc√™ √© um avaliador especializado em completude de User Stories derivadas de bugs.

BUG REPORT ORIGINAL:
{bug_report}

USER STORY GERADA:
{user_story}

USER STORY ESPERADA (Refer√™ncia):
{reference}

INSTRU√á√ïES:

Avalie a COMPLETUDE da user story em rela√ß√£o ao bug:

1. COBERTURA DO PROBLEMA (0.0 a 1.0):
   - A user story aborda TODOS os aspectos do bug reportado?
   - Nenhum detalhe importante foi omitido?
   - Se bug menciona m√∫ltiplos problemas, todos s√£o cobertos?

2. CONTEXTO T√âCNICO (0.0 a 1.0):
   - Quando o bug inclui detalhes t√©cnicos (logs, stack traces, endpoints):
     * User story preserva contexto t√©cnico relevante?
     * Informa√ß√µes t√©cnicas s√£o inclu√≠das de forma apropriada?
   - Bugs simples n√£o precisam de muito contexto t√©cnico
   - Bugs complexos DEVEM incluir se√ß√£o de contexto t√©cnico

3. IMPACTO E SEVERIDADE (0.0 a 1.0):
   - Se o bug menciona impacto (usu√°rios afetados, perda financeira):
     * User story reconhece e documenta o impacto?
   - Severidade √© refletida na prioriza√ß√£o impl√≠cita?
   - Bugs cr√≠ticos devem ter tratamento mais detalhado

4. TASKS T√âCNICAS (0.0 a 1.0):
   - Para bugs complexos com m√∫ltiplos componentes:
     * User story sugere tasks t√©cnicas ou breakdown?
   - Para bugs simples/m√©dios:
     * Tasks n√£o s√£o necess√°rias (n√£o penalizar aus√™ncia)
   - Avalie se o n√≠vel de detalhe √© apropriado √† complexidade

5. INFORMA√á√ïES ADICIONAIS RELEVANTES (0.0 a 1.0):
   - Se bug menciona: steps to reproduce, ambiente, logs
     * User story preserva ou referencia essas informa√ß√µes?
   - Contexto de neg√≥cio importante √© mantido?
   - Sugest√µes de solu√ß√£o s√£o apropriadas?

Calcule a M√âDIA dos 5 crit√©rios para obter o score final.

IMPORTANTE:
- Bugs SIMPLES podem ter score alto mesmo sem muitos detalhes t√©cnicos
- Bugs COMPLEXOS DEVEM ter se√ß√µes adicionais (contexto t√©cnico, tasks, impacto)
- Compare com a refer√™ncia para calibrar expectativa de completude

Retorne APENAS um objeto JSON v√°lido no formato:
{{
  "score": <valor entre 0.0 e 1.0>,
  "reasoning": "<explica√ß√£o detalhada sobre o que foi bem coberto e o que faltou, at√© 200 palavras>"
}}

N√ÉO adicione nenhum texto antes ou depois do JSON.
"""

    try:
        llm = get_evaluator_llm()
        response = invoke_with_throttle_retry(
            lambda: llm.invoke([HumanMessage(content=evaluator_prompt)]),
            context="evaluate_completeness_score"
        )
        result = extract_json_from_response(response.content)

        score = float(result.get("score", 0.0))

        return {
            "score": round(score, 4),
            "reasoning": result.get("reasoning", "")
        }

    except Exception as e:
        print(f"‚ùå Erro ao avaliar Completeness Score: {e}")
        return {
            "score": 0.0,
            "reasoning": f"Erro na avalia√ß√£o: {str(e)}"
        }


# Exemplo de uso e testes
if __name__ == "__main__":
    # Mostrar provider configurado
    provider = os.getenv("LLM_PROVIDER", "openai")
    eval_model = os.getenv("EVAL_MODEL", "gpt-4o")

    print("=" * 70)
    print("TESTANDO M√âTRICAS CUSTOMIZADAS")
    print("=" * 70)
    print(f"\nüìä Provider: {provider}")
    print(f"ü§ñ Modelo de Avalia√ß√£o: {eval_model}\n")

    print("=" * 70)
    print("PARTE 1: M√âTRICAS GERAIS")
    print("=" * 70)

    # Teste das m√©tricas gerais
    test_question = "Qual o hor√°rio de funcionamento da loja?"
    test_answer = "A loja funciona de segunda a sexta das 9h √†s 18h."
    test_reference = "Hor√°rio de funcionamento: Segunda a Sexta 9:00-18:00, S√°bado 9:00-14:00"

    print("\n1. F1-Score:")
    f1_result = evaluate_f1_score(test_question, test_answer, test_reference)
    print(f"   Score: {f1_result['score']:.2f}")
    print(f"   Precision: {f1_result['precision']:.2f}")
    print(f"   Recall: {f1_result['recall']:.2f}")
    print(f"   Reasoning: {f1_result['reasoning']}\n")

    print("2. Clarity:")
    clarity_result = evaluate_clarity(test_question, test_answer, test_reference)
    print(f"   Score: {clarity_result['score']:.2f}")
    print(f"   Reasoning: {clarity_result['reasoning']}\n")

    print("3. Precision:")
    precision_result = evaluate_precision(test_question, test_answer, test_reference)
    print(f"   Score: {precision_result['score']:.2f}")
    print(f"   Reasoning: {precision_result['reasoning']}\n")

    print("=" * 70)
    print("PARTE 2: M√âTRICAS ESPEC√çFICAS PARA BUG TO USER STORY")
    print("=" * 70)

    # Teste das m√©tricas espec√≠ficas de Bug to User Story
    test_bug = "Bot√£o de adicionar ao carrinho n√£o funciona no produto ID 1234."
    test_user_story = """Como um cliente navegando na loja, eu quero adicionar produtos ao meu carrinho de compras, para que eu possa continuar comprando e finalizar minha compra depois.

Crit√©rios de Aceita√ß√£o:
- Dado que estou visualizando um produto
- Quando clico no bot√£o "Adicionar ao Carrinho"
- Ent√£o o produto deve ser adicionado ao carrinho
- E devo ver uma confirma√ß√£o visual
- E o contador do carrinho deve ser atualizado"""

    test_reference_story = test_user_story  # Usando o mesmo para teste

    print("\n4. Tone Score (Tom profissional e emp√°tico):")
    tone_result = evaluate_tone_score(test_bug, test_user_story, test_reference_story)
    print(f"   Score: {tone_result['score']:.2f}")
    print(f"   Reasoning: {tone_result['reasoning']}\n")

    print("5. Acceptance Criteria Score (Qualidade dos crit√©rios):")
    criteria_result = evaluate_acceptance_criteria_score(test_bug, test_user_story, test_reference_story)
    print(f"   Score: {criteria_result['score']:.2f}")
    print(f"   Reasoning: {criteria_result['reasoning']}\n")

    print("6. User Story Format Score (Formato correto):")
    format_result = evaluate_user_story_format_score(test_bug, test_user_story, test_reference_story)
    print(f"   Score: {format_result['score']:.2f}")
    print(f"   Reasoning: {format_result['reasoning']}\n")

    print("7. Completeness Score (Completude e contexto):")
    completeness_result = evaluate_completeness_score(test_bug, test_user_story, test_reference_story)
    print(f"   Score: {completeness_result['score']:.2f}")
    print(f"   Reasoning: {completeness_result['reasoning']}\n")

    print("=" * 70)
    print("‚úÖ TODOS OS TESTES CONCLU√çDOS!")
    print("=" * 70)
